{"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":46346,"sourceType":"datasetVersion","datasetId":34662}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.applications import ResNet50\nfrom tensorflow.keras.applications.resnet50 import preprocess_input\nfrom PIL import Image\nimport numpy as np\nimport os\nfrom xml.etree import ElementTree as et\nfrom sklearn.model_selection import train_test_split","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-12T09:51:23.378469Z","iopub.execute_input":"2024-03-12T09:51:23.378914Z","iopub.status.idle":"2024-03-12T09:51:27.536239Z","shell.execute_reply.started":"2024-03-12T09:51:23.378881Z","shell.execute_reply":"2024-03-12T09:51:27.535349Z"},"id":"7j2riezzDbZF","trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import zipfile\n\n# Define the path to your zip file\nzip_file_path = '/content/archive.zip'\n\n# Define the directory where you want to extract the files\nextracted_dir_path = '/content/extracted_files/'\n\n# Create a ZipFile object\nwith zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n    # Extract all the contents of the zip file into the specified directory\n    zip_ref.extractall(extracted_dir_path)\n\n# Print a message indicating the extraction is complete\nprint(\"Extraction complete.\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aJs7VztwD-On","outputId":"ca0c3574-0e72-4636-fc3d-6638532e27e2"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":"Extraction complete.\n"}]},{"cell_type":"code","source":"# Define your root directories\ntrain_root = '/content/extracted_files/train_zip/train'\nval_root = '/content/extracted_files/test_zip/test'\n\n# Define your labels\nlabels = ['background', 'orange', 'apple', 'banana']\nlabel2targets = {l: t for t, l in enumerate(labels)}\nnum_classes = len(labels)","metadata":{"execution":{"iopub.status.busy":"2024-03-12T09:51:27.538120Z","iopub.execute_input":"2024-03-12T09:51:27.538671Z","iopub.status.idle":"2024-03-12T09:51:27.544342Z","shell.execute_reply.started":"2024-03-12T09:51:27.538645Z","shell.execute_reply":"2024-03-12T09:51:27.543260Z"},"id":"L57YPazuDbZI","trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import os\nimport numpy as np\nimport xml.etree.ElementTree as et\nfrom PIL import Image\n\ndef preprocess_img(img_path):\n    img = Image.open(img_path).convert('RGB')\n    img = img.resize((224, 224))  # Resize the image to a fixed size\n    img = np.array(img)\n    return img\n\ndef load_dataset(root):\n    images = []\n    targets = []\n    xml_paths = sorted([os.path.join(root, file) for file in os.listdir(root) if file.endswith('.xml')])\n    for xml_path in xml_paths:\n        img_path = xml_path.replace('.xml', '.jpg')\n        img = preprocess_img(img_path)\n        tree = et.parse(xml_path)\n        root = tree.getroot()\n        for obj in root.findall('object'):\n            label = obj.find('name').text\n            label_id = label2targets.get(label, 0)  # Assign 0 (background) if label not found\n            box = obj.find('bndbox')\n            xmin = int(box.find('xmin').text)\n            ymin = int(box.find('ymin').text)\n            xmax = int(box.find('xmax').text)\n            ymax = int(box.find('ymax').text)\n            targets.append([xmin, ymin, xmax, ymax, label_id])  # Store targets as a list\n            images.append(img)\n    return np.array(images), np.array(targets)\n\n# Define your train and validation root directories\ntrain_root = '/content/extracted_files/train_zip/train'\nval_root = '/content/extracted_files/train_zip/train'\n\n# Load the dataset\nX_train, y_train = load_dataset(train_root)\nX_val, y_val = load_dataset(val_root)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-12T09:51:27.546140Z","iopub.execute_input":"2024-03-12T09:51:27.546782Z","iopub.status.idle":"2024-03-12T09:51:32.601523Z","shell.execute_reply.started":"2024-03-12T09:51:27.546747Z","shell.execute_reply":"2024-03-12T09:51:32.600679Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"5UspoeOoDbZI","outputId":"ad82b0c5-8149-4789-aef0-3fa649903aef","trusted":true},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n\n  warnings.warn(\n"}]},{"cell_type":"code","source":"# Define your model\ndef get_model(num_classes):\n    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n    for layer in base_model.layers:\n        layer.trainable = False\n    x = layers.GlobalAveragePooling2D()(base_model.output)\n    x = layers.Dense(256, activation='relu')(x)\n    outputs = layers.Dense(num_classes)(x)\n    model = keras.Model(inputs=base_model.input, outputs=outputs)\n    return model\n\n# Compile your model\nmodel = get_model(num_classes)\nmodel.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n\n# Train your model\nmodel.fit(X_train, y_train[:, -1], epochs=30, batch_size=32, validation_data=(X_val, y_val[:, -1]))","metadata":{"execution":{"iopub.status.busy":"2024-03-12T09:51:32.602881Z","iopub.execute_input":"2024-03-12T09:51:32.603911Z","iopub.status.idle":"2024-03-12T09:52:25.812642Z","shell.execute_reply.started":"2024-03-12T09:51:32.603879Z","shell.execute_reply":"2024-03-12T09:52:25.811596Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"fYEDQYoGDbZJ","outputId":"9149b78d-f08e-4d35-8c1e-e59a85f22e7a","trusted":true},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n\n94765736/94765736 [==============================] - 5s 0us/step\n\nEpoch 1/30\n\n15/15 [==============================] - 13s 401ms/step - loss: 0.6656 - accuracy: 0.7634 - val_loss: 0.2288 - val_accuracy: 0.9054\n\nEpoch 2/30\n\n15/15 [==============================] - 3s 175ms/step - loss: 0.2493 - accuracy: 0.9075 - val_loss: 0.2179 - val_accuracy: 0.9118\n\nEpoch 3/30\n\n15/15 [==============================] - 3s 176ms/step - loss: 0.2804 - accuracy: 0.8968 - val_loss: 0.1907 - val_accuracy: 0.9140\n\nEpoch 4/30\n\n15/15 [==============================] - 3s 179ms/step - loss: 0.2518 - accuracy: 0.9054 - val_loss: 0.2066 - val_accuracy: 0.9097\n\nEpoch 5/30\n\n15/15 [==============================] - 4s 273ms/step - loss: 0.2010 - accuracy: 0.9226 - val_loss: 0.1686 - val_accuracy: 0.9204\n\nEpoch 6/30\n\n15/15 [==============================] - 3s 177ms/step - loss: 0.2191 - accuracy: 0.9161 - val_loss: 0.1944 - val_accuracy: 0.9204\n\nEpoch 7/30\n\n15/15 [==============================] - 3s 178ms/step - loss: 0.1948 - accuracy: 0.9183 - val_loss: 0.2133 - val_accuracy: 0.9140\n\nEpoch 8/30\n\n15/15 [==============================] - 3s 179ms/step - loss: 0.2274 - accuracy: 0.9054 - val_loss: 0.1776 - val_accuracy: 0.9183\n\nEpoch 9/30\n\n15/15 [==============================] - 4s 271ms/step - loss: 0.2030 - accuracy: 0.9032 - val_loss: 0.1637 - val_accuracy: 0.9161\n\nEpoch 10/30\n\n15/15 [==============================] - 3s 179ms/step - loss: 0.2004 - accuracy: 0.9075 - val_loss: 0.1577 - val_accuracy: 0.9226\n\nEpoch 11/30\n\n15/15 [==============================] - 3s 179ms/step - loss: 0.2263 - accuracy: 0.9097 - val_loss: 0.2193 - val_accuracy: 0.9204\n\nEpoch 12/30\n\n15/15 [==============================] - 3s 180ms/step - loss: 0.2388 - accuracy: 0.9075 - val_loss: 0.1717 - val_accuracy: 0.9183\n\nEpoch 13/30\n\n15/15 [==============================] - 3s 185ms/step - loss: 0.2213 - accuracy: 0.9054 - val_loss: 0.1692 - val_accuracy: 0.9204\n\nEpoch 14/30\n\n15/15 [==============================] - 3s 182ms/step - loss: 0.1939 - accuracy: 0.9011 - val_loss: 0.1645 - val_accuracy: 0.9161\n\nEpoch 15/30\n\n15/15 [==============================] - 3s 180ms/step - loss: 0.1821 - accuracy: 0.9118 - val_loss: 0.1755 - val_accuracy: 0.9140\n\nEpoch 16/30\n\n15/15 [==============================] - 3s 180ms/step - loss: 0.2176 - accuracy: 0.9075 - val_loss: 0.1495 - val_accuracy: 0.9226\n\nEpoch 17/30\n\n15/15 [==============================] - 4s 272ms/step - loss: 0.2243 - accuracy: 0.9097 - val_loss: 0.2323 - val_accuracy: 0.9032\n\nEpoch 18/30\n\n15/15 [==============================] - 3s 181ms/step - loss: 0.2299 - accuracy: 0.9075 - val_loss: 0.1628 - val_accuracy: 0.9161\n\nEpoch 19/30\n\n15/15 [==============================] - 4s 272ms/step - loss: 0.1837 - accuracy: 0.9269 - val_loss: 0.2507 - val_accuracy: 0.9075\n\nEpoch 20/30\n\n15/15 [==============================] - 3s 184ms/step - loss: 0.2058 - accuracy: 0.9075 - val_loss: 0.1701 - val_accuracy: 0.9204\n\nEpoch 21/30\n\n15/15 [==============================] - 3s 190ms/step - loss: 0.2172 - accuracy: 0.9204 - val_loss: 0.2378 - val_accuracy: 0.9032\n\nEpoch 22/30\n\n15/15 [==============================] - 3s 185ms/step - loss: 0.2129 - accuracy: 0.9011 - val_loss: 0.1648 - val_accuracy: 0.9118\n\nEpoch 23/30\n\n15/15 [==============================] - 3s 185ms/step - loss: 0.1850 - accuracy: 0.9054 - val_loss: 0.1568 - val_accuracy: 0.9226\n\nEpoch 24/30\n\n15/15 [==============================] - 4s 274ms/step - loss: 0.1743 - accuracy: 0.9075 - val_loss: 0.1867 - val_accuracy: 0.9204\n\nEpoch 25/30\n\n15/15 [==============================] - 3s 191ms/step - loss: 0.1670 - accuracy: 0.9161 - val_loss: 0.1526 - val_accuracy: 0.9204\n\nEpoch 26/30\n\n15/15 [==============================] - 3s 186ms/step - loss: 0.1662 - accuracy: 0.9118 - val_loss: 0.1495 - val_accuracy: 0.9226\n\nEpoch 27/30\n\n15/15 [==============================] - 3s 186ms/step - loss: 0.1655 - accuracy: 0.9075 - val_loss: 0.1461 - val_accuracy: 0.9204\n\nEpoch 28/30\n\n15/15 [==============================] - 4s 275ms/step - loss: 0.1691 - accuracy: 0.9140 - val_loss: 0.1565 - val_accuracy: 0.9118\n\nEpoch 29/30\n\n15/15 [==============================] - 3s 191ms/step - loss: 0.1618 - accuracy: 0.9011 - val_loss: 0.1482 - val_accuracy: 0.9226\n\nEpoch 30/30\n\n15/15 [==============================] - 4s 275ms/step - loss: 0.1676 - accuracy: 0.9011 - val_loss: 0.1675 - val_accuracy: 0.9204\n"},{"output_type":"execute_result","execution_count":8,"data":{"text/plain":["<keras.src.callbacks.History at 0x7af4500cf130>"]},"metadata":{}}]},{"cell_type":"code","source":"# Evaluate your model\nmodel.evaluate(X_val, y_val[:, -1])\n\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\ntflite_model = converter.convert()\n\n# Save the TensorFlow Lite model to a file\nwith open('my_model.tflite', 'wb') as f:\n    f.write(tflite_model)\n\nprint('end')","metadata":{"execution":{"iopub.status.busy":"2024-03-12T09:52:25.815564Z","iopub.execute_input":"2024-03-12T09:52:25.816385Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"47J6qV0iDbZJ","outputId":"00eab553-8aab-427a-dfec-f3d3e994d6a0","trusted":true},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":"15/15 [==============================] - 1s 95ms/step - loss: 0.1675 - accuracy: 0.9204\n\nend\n"}]}]}